# Autopilot Project

## Overview
This project implements an AI-powered autopilot End-to-End ML system for autonomous vehicles, featuring data collection, model training, and real-time inference. It includes tools for processing driving data, training world models, and running both simulated and real-time control loops using deep learning models.

## Features
- **Data Processing:** Extracts images and steering data from ROS2 bag files for training.
- **World Model Inference:** Uses trained neural networks (VAE encoder/decoder, RNN, controller) for simulating and controlling vehicle behavior.
- **Dream Racer Simulation:** Visualizes the world model and allows manual/autopilot control in a simulated environment.
- **ROS2 Integration:** Real-time inference and control using ROS2 topics and custom messages.
- **Flight Recorder:** Logs inference and control data for analysis.

## Architecture (World Model)
Instead of a standard Convolutional Neural Network (CNN) that connects pixels directly to steering, A World Model approach is used by splitting the brain into parts: **Vision** and **Control**

### 1. Vision System (Variational Autoencoder, VAE)
The robot's camera captures 12,800 pixels (dots) every frame. Most of this is useless noise (the texture of the wood floor, the shadow of the table, the color of the wall).
The VAE learns to ignore the noise, extract main signal (geometry) and summarize the entire image into just **32 numbers**. These 32 numbers represent the "Essence" of the road (e.g., "Road curves left," "Road is straight").

Trained a Convolutional Encoder to map the input image $(160 \times 80 \times 3)$ to a **Latent Vector $z$** $(1 \times 32)$. This forces the model to learn a disentangled representation of the environment, filtering out high-frequency noise and focusing on geometry.

### 2. The Controller System (DNN)
A Multi-Layer Perceptron (MLP) that maps the Latent Vector $z$ to a Steering Command $\hat{y}$. This is a "Latent Policy.". It doesn't look at the video feed; it looks at the **32 numbers** generated by the Vision System. It learns patterns like: "When the numbers look like *this*, the human steers *left*." Because it looks at simple numbers instead of complex images, it reacts incredibly fast.
By decoupling perception from control, the driving policy becomes robust to visual noise.

### 3. The Simulation (Ghost Drive)
Before putting the code on the robot, trained a Memory Model (RNN) to "dream."

To prove that the AI understood the physics of driving, an MDN-RNN model is trained to predict the latent space at next timestep given the current time latent space (state) and action $P(z_{t+1} | z_t, a_t)$. 

Simply put: It is like asking the AI: *"If you are here, and you steer left, what do you think you will see next?"*. 

The output: The AI successfully generated a video from scratch (hallucinating the future) showing the track moving correctly. This proved the AI understood the physics of driving.

This validated that our Latent Space $z$ captured sufficient temporal and spatial information to reconstruct the car's dynamics.

<details>
	<summary>Dream Run Demo (Click to play/pause)</summary>
	<video src="https://github.com/user-attachments/assets/dbea051e-28a8-4de5-9e38-787ee6b8a432" controls width="640" height="360">
		Your browser does not support the video tag.
	</video>
</details>

https://github.com/user-attachments/assets/dbea051e-28a8-4de5-9e38-787ee6b8a432

### 4. Reasoning and Reacting to Objects (to be added) 

### Applying Smoothing and Dynamic Braking
*   **Quantization:** Converted the models to `.tflite` for edge inference (Raspberry PI 5).
*   **Inference Speed:** Achieved ~4ms latency per frame.
*   **Control Theory:** Implemented a non-linear steering gain ($y = x^{0.85}$) to increase sensitivity in turns, and a deadzone to prevent oscillation on straights.


## Directory Structure
```
├── autopilot_new.py
├── autopilot_wm.py
├── autopilot.py (ignore)
├── dream_racer.py
├── process_data.py
├── training_images/
├── *.keras (model files)
├── *.tflite (TensorFlow Lite models)
├── README.md
```

## Main Components
- **process_data.py:** Extracts images and steering labels from ROS2 bag files and saves them for training.
- **dream_racer.py:** Simulates the world model, visualizes predictions, and allows manual/autopilot control (you can use this file to test in simulation - since you might not have access to the physical robot).
- **autopilot_wm.py:** ROS2 node for real-time inference and control using TensorFlow Lite models.

## Setup
1. **Install Dependencies:**
	- Python 3.10+
	- TensorFlow, OpenCV, NumPy, pandas, ROS2, cv_bridge
	- Custom wheels: `ai_edge_litert`, `tflite_runtime` (provided in repo - ignore)
	- Other Python packages as needed (see `requirements.txt`)

2. **Prepare Data:**
	- Place ROS2 bag folders in the project directory.
	- Run `process_data.py` to extract images and steering data:
	  ```bash
	  python process_data.py
	  ```
	- Output: `training_images/` and `driving_log.csv`
    
    Note: If you need access to the dataset, feel free to reach out (LinkedIn or Twitter/X). I can't upload to github because of the size.  

3. **Train Models:**
	- Use your preferred training pipeline to train VAE encoder/decoder, RNN, and controller models.
	- Save models as `.keras` and convert to `.tflite` for deployment.
    - You can download all models from the releases here on Github. 

4. **Run Dream Racer Simulation:**
	```bash
	python dream_racer.py
	```
	- Controls:
	  - `A`: Toggle Autopilot
	  - `M`: Manual Mode
	  - `J/L`: Steer Left/Right
	  - `R`: Reset
	  - `Q`: Quit
    
    We have a Red/Green bar so you can see how hard the robot is steering. In Manual Mode, it shows a Blue Dot representing what the Autopilot would do if it were in charge.

5. **Deploy on ROS2 Robot:**
	- Launch `autopilot_wm.py` as a ROS2 node for real-time control.

## File Descriptions
- **autopilot_new.py / autopilot.py:** Alternative or legacy autopilot implementations.
- **autopilot_wm.py:** World model autopilot for ROS2 integration (To be run in the physical Robot).
- **dream_racer.py:** Simulated world model and control loop.
- **process_data.py:** Data extraction from ROS2 bags.
- **training_images/:** Extracted images for training.
- **training_data_version/:** Versioned model files.
- ***.keras / *.tflite:** Trained model files.
- **driving_log.csv:** Steering labels for training.

## Notes
- Ensure all model files are present before running inference scripts.
- For ROS2 integration, install all required ROS2 packages and custom message types.
- For TensorFlow Lite inference, use the provided wheels for ARM/aarch64 platforms.

## License
This project is provided for educational and research purposes. See LICENSE for details (to be added).

## Acknowledgements
- ROS2
- TensorFlow
- OpenCV
- Contributors to open-source driving datasets and world model research
